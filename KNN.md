---
title: 机器学习之KNN分类算法详解
date: 2021-02-10 20:21:46
categories:
	- machine learning
tags:
	- classifier
---

## 前言

KNN（K-Nearest-Neighbor）算法，也就是广为人知的K近邻算法，可以说是最简单的数据分类方法之一。它是一种监督学习（存在训练样本）、懒惰学习（在测试样本阶段学习）的分类方法，基于训练集对预测集进行预测。本篇介绍KNN很大程度上基于sklearn(scikit-learn)，一个python中常用的机器学习库。

------

## 基本原理

KNN的基本原理并不复杂。其思路颇为直观，就是选择在训练集中离需要分类的点最近的k个点，结合这k个点的信息将其通过某种分类规则判定为某个类别。

值得注意的是，虽然KNN算法通常使用欧式距离进行计算，但并非所有都应采用欧式（例如可以使用曼哈顿距离等）。

------

## sklearn中的使用

```python
def KNeighborsClassifier(n_neighbors = 5,
                       weights='uniform',
                       algorithm = '',
                       leaf_size = '30',
                       p = 2,
                       metric = 'minkowski',
                       metric_params = None,
                       n_jobs = None
                       )
```

- n_neighbors：这个值就是指 KNN 中的 “K”了。前面说到过，通过调整 K 值，算法会有不同的效果。

- weights（权重）：最普遍的 KNN 算法无论距离如何，权重都一样，但有时候我们想搞点特殊化，比如距离更近的点让它更加重要。这时候就需要 weight 这个参数了，这个参数有三个可选参数的值，决定了如何分配权重。参数选项如下：

    - 'uniform'：不管远近权重都一样，就是最普通的 KNN 算法的形式。
    - 'distance'：权重和距离成反比，距离预测目标越近具有越高的权重。
    - 自定义函数：自定义一个函数，根据输入的坐标值返回对应的权重，达到自定义权重的目的。

- algorithm：在 sklearn 中，要构建 KNN 模型有三种构建方式，1. 暴力法，就是直接计算距离存储比较的那种放松。2. 使用 kd 树构建 KNN 模型 3. 使用球树构建。 其中暴力法适合数据较小的方式，否则效率会比较低。如果数据量比较大一般会选择用 KD 树构建 KNN 模型，而当 KD 树也比较慢的时候，则可以试试球树来构建 KNN。参数选项如下：
	
	- 'brute' ：蛮力实现
	- 'kd_tree'：KD 树实现 KNN
	- 'ball_tree'：球树实现 KNN 
- 'auto'： 默认参数，自动选择合适的方法构建模型
	
	不过当数据较小或比较稀疏时，无论选择哪个最后都会使用 'brute'
	
- leaf_size：如果是选择蛮力实现，那么这个值是可以忽略的，当使用KD树或球树，它就是是停止建子树的叶子节点数量的阈值。默认30，但如果数据量增多这个参数需要增大，否则速度过慢不说，还容易过拟合。

- p：和metric结合使用的，当metric参数是"minkowski"的时候，p=1为曼哈顿距离， p=2为欧式距离。默认为p=2。

- metric：指定距离度量方法，一般都是使用欧式距离。
	
	- 'euclidean' ：欧式距离
	- 'manhattan'：曼哈顿距离
	- 'chebyshev'：切比雪夫距离
	- 'minkowski'： 闵可夫斯基距离，默认参数
	
- n_jobs：指定多少个CPU进行运算，默认是-1，也就是全部都算。

---

## 算法要素

我们通常认为，在一个KNN算法中，需要考量三个要素：

- 距离度量方式
- K值选择
- 分类规则

### 距离度量方式

首先来谈一谈KNN算法中距离度量方式的选择。在sklearn中，KNN分类器提供了四种距离，我在此一一介绍一下。

假设存在两个$n$维向量$\vec{x}$和$\vec{y}$，$x_i$表示$\vec{x}$第$i$维度的值，$y_i$同理，则两者之间的距离可以表示为：

1. 欧式距离(euclidean) $D_e$。作为最直观也是最通用的距离度量，欧式距离几乎可以满足大多数需要使用KNN进行分类的情况。其计算公式为:
   $$
   D_e(\vec{x},\vec{y})=\sqrt{\sum^n_{i=1}(x_i-y_i)^2}
   $$

   sklearn中默认使用的就是欧式距离，大多数情况下不用太操心距离度量，采用欧氏距离即可。

2. 曼哈顿距离(manhatten) $D_{man}$。曼哈顿距离也是数学中比较常用的距离度量之一，用以表示标准坐标系下两点之间的轴距和。其计算公式为：
   $$
   D_{man}(\vec{x}, \vec{y})=\sum^n_{i=1}|x_i-y_i|
   $$

3. 切比雪夫距离(chebyshev) $D_c$。切比雪夫距离和曼哈顿距离类似，但其采用的是两点之间的最大轴距。其计算公式为:
   $$
   D_c(\vec{x},\vec{y})=\max{(|x_i-y_i|), i\in[1,n]}
   $$

4. 闵可夫斯基距离(minkowski) $D_{min}$。闵氏空间是狭义相对论中的概念，指代一个时间维度和三个空间维度(3+1)组成的时空，其中光速在各个参考系内恒为定制。因此，闵可夫斯基距离有时又指代时空距离。其计算公式为：
   $$
   D_{min}(\vec{x}, \vec{y})=(\sum^n_{i=1}|x_i-y_i|^p)^\frac{1}{p}
   $$
   我们可以看到，在$p=1$时，闵氏距离表现为曼哈顿距离；在$p=2$时，闵氏距离表现为欧氏距离；注意，在$p\to \inf$时，闵氏距离表现为切比雪夫距离。

当然，仍然还有许多距离度量可以采用，例如马哈拉诺比斯距离，但由于sklearn并未提供，这里就不再赘述。在实际问题中，往往欧氏距离就能得到很好的效果，不需要考虑其他的度量方式。

### K值选择

在KNN分类中，K值的选择往往没有一个固定的经验，可以通过不停调整(例如交叉验证)到一个合适的K值。

1. K为1。如果K值被设定为1，那么训练集的正确率将达到100%(将训练集同时作为预测集)，因为每个点只会找到它本身，但同时在预测集中的正确率不会太高（极度过拟合）。

2. K为较小的值。较小的邻域往往会带来更低的训练误差，但会导致过拟合的问题降低预测集的准确率。
3. K为较大的值。较大的邻域会增大训练误差，但能够有效减少过拟合的问题。（注意，这并不意味着预测集的准确率一定会增加）
4. K为训练集样本数量。当K极端到邻域覆盖整个样本时，就相当于不再分类而直接选择在训练集中出现最多的类。

### 分类决策规则

在选出了最近的K个点后，往往仍然需要使用某种分类规则来决出类别。在分类预测的时候，我们一般采用多数表决法（sklearn采用的方法）；而在KNN回归的时候，才会使用选择平均法。由于本篇不涉及KNN在回归中的使用，因此仅介绍多数表决法。

多数表决法就是采取所有对象中最多的那个。在KNN分类情境中，就是选择K个点中出现最多的那个类别。例如最相邻的5个点中有3个A类，1个B类，1个C类，那么这个点就会被分为A类。

------

## 算法实现

在决定好以上三个要素之后，问题就来到了，我们应该如何选择K个最近点。sklearn为KNN提供了三种实现方法（当然，还有一种自动选择用的auto）：

1. brute。Brute Force，也就是经典的暴力枚举，使用遍历或者矩阵乘法的方式计算两点之间的距离并找到k个最近点（可以考虑使用scipy.spatial.distance.cdist和numpy.argpartition函数）。在较小的训练集中有比较好的表现，但在大训练集中会消耗较多时间，可以考虑下面两种方法。
2. kd_tree。kd树，一种经典的高维数据查询结构，对空间以超矩形体进行划分。
3. ball_tree。球树。球树与kd树类似，但对空间以超球体进行划分以避免一些不必要的搜索。

这里并没有详细介绍各个算法的具体实现方式，日后可能在新的文章里会提到。

------

## KNN的优劣

### 优点

1. KNN是一种较为成熟的算法，同时思路也比较简单，能够同时兼容回归与分类（KNN的回归将在日后的回归算法中提到）。
2. KNN时间复杂度为O(n)。因为是懒惰学习，在训练阶段速度比较快。
3. 可以用于非线性分类。
4. 未对数据进行任何假设，因此对异常点不敏感。
5. 通过近邻而不是通过类域判别，对类域交叉重叠较多的样本具有较好的预测效果。

### 缺点

1. 在特征较多的情况下，会有很大的计算量。
2. 需要存储所有的训练数据，对内存要求高。
3. 因为是懒惰学习，预测阶段速度比较慢。
4. 在样本不平衡时，容易造成误判。
5. 对数据规模敏感。在大的训练集中有较高正确率，当规模小的时候正确率低。

------

## KNN的衍生

1. 带权重的距离。在KNN算法中，无论距离远近，其权重都是一致的。因此，sklearn提供了weights参数来根据距离调整权重。
2. 限定半径最近邻分类。在选择点的时候，不同于选择固定的K个，而是查找一个半径内的所有相邻点。具体使用可以查看sklearn.neighbors.RadiusNeighborsClassifier的说明。

---

## 参考资料

1. KNN（K-Nearest Neighbor）最近邻规则分类, caiqingfei, https://www.cnblogs.com/rrttp/articles/8032690.html
2. 深入浅出KNN算法（一） KNN算法原理, zzzzMing, https://www.cnblogs.com/listenfwind/p/10311496.html
3. 深入浅出KNN算法（二） sklearn KNN实践, zzzzMing, https://www.cnblogs.com/listenfwind/p/10685192.html
4. K近邻法(KNN)原理小结, 刘建平Pinard, https://www.cnblogs.com/pinard/p/6061661.html
5. KD树, kexinxin, https://www.cnblogs.com/kexinxin/p/11795447.html

---

Author：Hitomichi

<span style="color: red">注：欢迎转载，请标明出处</span>